{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1G7WUhuzNyLqElywWxQTWD9yFgM-KtqYg",
      "authorship_tag": "ABX9TyO9t0MEdP5sddssNrr8GIgx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kavyapm1960/project/blob/main/EN_FR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQHKr9oy1Q0N",
        "outputId": "31126aa7-3afe-4819-cfc6-4961177e8efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "216/216 [==============================] - 27s 91ms/step - loss: 2.1610 - accuracy: 0.0913 - val_loss: 1.9537 - val_accuracy: 0.1212\n",
            "Epoch 2/200\n",
            "216/216 [==============================] - 16s 75ms/step - loss: 1.8323 - accuracy: 0.1321 - val_loss: 1.6791 - val_accuracy: 0.1585\n",
            "Epoch 3/200\n",
            "216/216 [==============================] - 16s 75ms/step - loss: 1.5969 - accuracy: 0.1786 - val_loss: 1.5082 - val_accuracy: 0.2155\n",
            "Epoch 4/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 1.4443 - accuracy: 0.2151 - val_loss: 1.3733 - val_accuracy: 0.2272\n",
            "Epoch 5/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 1.3152 - accuracy: 0.2479 - val_loss: 1.2360 - val_accuracy: 0.2646\n",
            "Epoch 6/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 1.2022 - accuracy: 0.2700 - val_loss: 1.1694 - val_accuracy: 0.2788\n",
            "Epoch 7/200\n",
            "216/216 [==============================] - 16s 74ms/step - loss: 1.1135 - accuracy: 0.2861 - val_loss: 1.0663 - val_accuracy: 0.2954\n",
            "Epoch 8/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 1.0521 - accuracy: 0.2977 - val_loss: 1.0089 - val_accuracy: 0.3048\n",
            "Epoch 9/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 1.0025 - accuracy: 0.3079 - val_loss: 0.9798 - val_accuracy: 0.3154\n",
            "Epoch 10/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.9611 - accuracy: 0.3176 - val_loss: 0.9357 - val_accuracy: 0.3223\n",
            "Epoch 11/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.9263 - accuracy: 0.3262 - val_loss: 0.9035 - val_accuracy: 0.3345\n",
            "Epoch 12/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.8966 - accuracy: 0.3330 - val_loss: 0.8755 - val_accuracy: 0.3405\n",
            "Epoch 13/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.8681 - accuracy: 0.3394 - val_loss: 0.8427 - val_accuracy: 0.3467\n",
            "Epoch 14/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.8404 - accuracy: 0.3483 - val_loss: 0.8236 - val_accuracy: 0.3527\n",
            "Epoch 15/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.8116 - accuracy: 0.3573 - val_loss: 0.7881 - val_accuracy: 0.3628\n",
            "Epoch 16/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.7870 - accuracy: 0.3639 - val_loss: 0.7769 - val_accuracy: 0.3661\n",
            "Epoch 17/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.7684 - accuracy: 0.3677 - val_loss: 0.7521 - val_accuracy: 0.3696\n",
            "Epoch 18/200\n",
            "216/216 [==============================] - 16s 73ms/step - loss: 0.7523 - accuracy: 0.3717 - val_loss: 0.7439 - val_accuracy: 0.3772\n",
            "Epoch 19/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.7386 - accuracy: 0.3748 - val_loss: 0.7197 - val_accuracy: 0.3771\n",
            "Epoch 20/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.7239 - accuracy: 0.3789 - val_loss: 0.7262 - val_accuracy: 0.3771\n",
            "Epoch 21/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.7112 - accuracy: 0.3829 - val_loss: 0.7087 - val_accuracy: 0.3830\n",
            "Epoch 22/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.6993 - accuracy: 0.3868 - val_loss: 0.6824 - val_accuracy: 0.3899\n",
            "Epoch 23/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.6865 - accuracy: 0.3912 - val_loss: 0.6796 - val_accuracy: 0.3929\n",
            "Epoch 24/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.6755 - accuracy: 0.3952 - val_loss: 0.6660 - val_accuracy: 0.3974\n",
            "Epoch 25/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.6637 - accuracy: 0.3998 - val_loss: 0.6647 - val_accuracy: 0.3991\n",
            "Epoch 26/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.6512 - accuracy: 0.4047 - val_loss: 0.6466 - val_accuracy: 0.4058\n",
            "Epoch 27/200\n",
            "216/216 [==============================] - 16s 73ms/step - loss: 0.6386 - accuracy: 0.4098 - val_loss: 0.6293 - val_accuracy: 0.4138\n",
            "Epoch 28/200\n",
            "216/216 [==============================] - 17s 78ms/step - loss: 0.6256 - accuracy: 0.4146 - val_loss: 0.6227 - val_accuracy: 0.4120\n",
            "Epoch 29/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.6125 - accuracy: 0.4194 - val_loss: 0.6184 - val_accuracy: 0.4190\n",
            "Epoch 30/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.6004 - accuracy: 0.4228 - val_loss: 0.5885 - val_accuracy: 0.4260\n",
            "Epoch 31/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.5895 - accuracy: 0.4255 - val_loss: 0.5843 - val_accuracy: 0.4269\n",
            "Epoch 32/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.5794 - accuracy: 0.4282 - val_loss: 0.5781 - val_accuracy: 0.4267\n",
            "Epoch 33/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.5714 - accuracy: 0.4303 - val_loss: 0.5655 - val_accuracy: 0.4305\n",
            "Epoch 34/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.5651 - accuracy: 0.4319 - val_loss: 0.5582 - val_accuracy: 0.4321\n",
            "Epoch 35/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.5586 - accuracy: 0.4330 - val_loss: 0.5510 - val_accuracy: 0.4339\n",
            "Epoch 36/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.5516 - accuracy: 0.4346 - val_loss: 0.5510 - val_accuracy: 0.4350\n",
            "Epoch 37/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.5467 - accuracy: 0.4355 - val_loss: 0.5550 - val_accuracy: 0.4340\n",
            "Epoch 38/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.5420 - accuracy: 0.4369 - val_loss: 0.5406 - val_accuracy: 0.4350\n",
            "Epoch 39/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.5368 - accuracy: 0.4378 - val_loss: 0.5304 - val_accuracy: 0.4382\n",
            "Epoch 40/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.5320 - accuracy: 0.4388 - val_loss: 0.5270 - val_accuracy: 0.4372\n",
            "Epoch 41/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.5270 - accuracy: 0.4397 - val_loss: 0.5214 - val_accuracy: 0.4383\n",
            "Epoch 42/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.5252 - accuracy: 0.4400 - val_loss: 0.5156 - val_accuracy: 0.4408\n",
            "Epoch 43/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.5185 - accuracy: 0.4416 - val_loss: 0.5182 - val_accuracy: 0.4401\n",
            "Epoch 44/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.5151 - accuracy: 0.4421 - val_loss: 0.5120 - val_accuracy: 0.4417\n",
            "Epoch 45/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.5111 - accuracy: 0.4433 - val_loss: 0.5108 - val_accuracy: 0.4420\n",
            "Epoch 46/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.5079 - accuracy: 0.4440 - val_loss: 0.5104 - val_accuracy: 0.4422\n",
            "Epoch 47/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.5051 - accuracy: 0.4444 - val_loss: 0.4969 - val_accuracy: 0.4452\n",
            "Epoch 48/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.5017 - accuracy: 0.4448 - val_loss: 0.4921 - val_accuracy: 0.4449\n",
            "Epoch 49/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4989 - accuracy: 0.4456 - val_loss: 0.4915 - val_accuracy: 0.4459\n",
            "Epoch 50/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4963 - accuracy: 0.4461 - val_loss: 0.4896 - val_accuracy: 0.4447\n",
            "Epoch 51/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4929 - accuracy: 0.4466 - val_loss: 0.4943 - val_accuracy: 0.4459\n",
            "Epoch 52/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4902 - accuracy: 0.4471 - val_loss: 0.4864 - val_accuracy: 0.4469\n",
            "Epoch 53/200\n",
            "216/216 [==============================] - 17s 78ms/step - loss: 0.4879 - accuracy: 0.4476 - val_loss: 0.4807 - val_accuracy: 0.4471\n",
            "Epoch 54/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4847 - accuracy: 0.4484 - val_loss: 0.4798 - val_accuracy: 0.4466\n",
            "Epoch 55/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4833 - accuracy: 0.4486 - val_loss: 0.4839 - val_accuracy: 0.4484\n",
            "Epoch 56/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4805 - accuracy: 0.4496 - val_loss: 0.4799 - val_accuracy: 0.4504\n",
            "Epoch 57/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4780 - accuracy: 0.4504 - val_loss: 0.4795 - val_accuracy: 0.4490\n",
            "Epoch 58/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4758 - accuracy: 0.4510 - val_loss: 0.4754 - val_accuracy: 0.4523\n",
            "Epoch 59/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4734 - accuracy: 0.4522 - val_loss: 0.4703 - val_accuracy: 0.4516\n",
            "Epoch 60/200\n",
            "216/216 [==============================] - 17s 76ms/step - loss: 0.4707 - accuracy: 0.4528 - val_loss: 0.4675 - val_accuracy: 0.4530\n",
            "Epoch 61/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4683 - accuracy: 0.4536 - val_loss: 0.4750 - val_accuracy: 0.4527\n",
            "Epoch 62/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4661 - accuracy: 0.4541 - val_loss: 0.4665 - val_accuracy: 0.4532\n",
            "Epoch 63/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4642 - accuracy: 0.4547 - val_loss: 0.4549 - val_accuracy: 0.4521\n",
            "Epoch 64/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4615 - accuracy: 0.4553 - val_loss: 0.4596 - val_accuracy: 0.4545\n",
            "Epoch 65/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4591 - accuracy: 0.4560 - val_loss: 0.4594 - val_accuracy: 0.4522\n",
            "Epoch 66/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4571 - accuracy: 0.4565 - val_loss: 0.4594 - val_accuracy: 0.4536\n",
            "Epoch 67/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4552 - accuracy: 0.4571 - val_loss: 0.4550 - val_accuracy: 0.4569\n",
            "Epoch 68/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4525 - accuracy: 0.4580 - val_loss: 0.4509 - val_accuracy: 0.4567\n",
            "Epoch 69/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.4507 - accuracy: 0.4586 - val_loss: 0.4500 - val_accuracy: 0.4589\n",
            "Epoch 70/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.4486 - accuracy: 0.4592 - val_loss: 0.4521 - val_accuracy: 0.4557\n",
            "Epoch 71/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4467 - accuracy: 0.4598 - val_loss: 0.4398 - val_accuracy: 0.4578\n",
            "Epoch 72/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4448 - accuracy: 0.4605 - val_loss: 0.4389 - val_accuracy: 0.4593\n",
            "Epoch 73/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.4428 - accuracy: 0.4609 - val_loss: 0.4412 - val_accuracy: 0.4603\n",
            "Epoch 74/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4402 - accuracy: 0.4615 - val_loss: 0.4444 - val_accuracy: 0.4586\n",
            "Epoch 75/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4386 - accuracy: 0.4621 - val_loss: 0.4366 - val_accuracy: 0.4597\n",
            "Epoch 76/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.4364 - accuracy: 0.4625 - val_loss: 0.4330 - val_accuracy: 0.4616\n",
            "Epoch 77/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4343 - accuracy: 0.4632 - val_loss: 0.4326 - val_accuracy: 0.4626\n",
            "Epoch 78/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4318 - accuracy: 0.4637 - val_loss: 0.4295 - val_accuracy: 0.4616\n",
            "Epoch 79/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.4302 - accuracy: 0.4638 - val_loss: 0.4265 - val_accuracy: 0.4625\n",
            "Epoch 80/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4288 - accuracy: 0.4638 - val_loss: 0.4313 - val_accuracy: 0.4642\n",
            "Epoch 81/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4252 - accuracy: 0.4646 - val_loss: 0.4323 - val_accuracy: 0.4631\n",
            "Epoch 82/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4237 - accuracy: 0.4650 - val_loss: 0.4251 - val_accuracy: 0.4620\n",
            "Epoch 83/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4219 - accuracy: 0.4648 - val_loss: 0.4236 - val_accuracy: 0.4636\n",
            "Epoch 84/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4190 - accuracy: 0.4657 - val_loss: 0.4223 - val_accuracy: 0.4636\n",
            "Epoch 85/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4169 - accuracy: 0.4661 - val_loss: 0.4179 - val_accuracy: 0.4634\n",
            "Epoch 86/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4154 - accuracy: 0.4663 - val_loss: 0.4148 - val_accuracy: 0.4643\n",
            "Epoch 87/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.4136 - accuracy: 0.4668 - val_loss: 0.4128 - val_accuracy: 0.4639\n",
            "Epoch 88/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4111 - accuracy: 0.4669 - val_loss: 0.4107 - val_accuracy: 0.4661\n",
            "Epoch 89/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4096 - accuracy: 0.4673 - val_loss: 0.4131 - val_accuracy: 0.4663\n",
            "Epoch 90/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4076 - accuracy: 0.4680 - val_loss: 0.4104 - val_accuracy: 0.4664\n",
            "Epoch 91/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4060 - accuracy: 0.4683 - val_loss: 0.4089 - val_accuracy: 0.4652\n",
            "Epoch 92/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.4042 - accuracy: 0.4685 - val_loss: 0.4153 - val_accuracy: 0.4648\n",
            "Epoch 93/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.4027 - accuracy: 0.4685 - val_loss: 0.4001 - val_accuracy: 0.4643\n",
            "Epoch 94/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.4002 - accuracy: 0.4690 - val_loss: 0.4000 - val_accuracy: 0.4661\n",
            "Epoch 95/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3988 - accuracy: 0.4694 - val_loss: 0.3965 - val_accuracy: 0.4682\n",
            "Epoch 96/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3970 - accuracy: 0.4696 - val_loss: 0.3972 - val_accuracy: 0.4648\n",
            "Epoch 97/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3955 - accuracy: 0.4702 - val_loss: 0.3960 - val_accuracy: 0.4666\n",
            "Epoch 98/200\n",
            "216/216 [==============================] - 17s 76ms/step - loss: 0.3937 - accuracy: 0.4702 - val_loss: 0.3945 - val_accuracy: 0.4675\n",
            "Epoch 99/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3920 - accuracy: 0.4704 - val_loss: 0.3947 - val_accuracy: 0.4676\n",
            "Epoch 100/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3906 - accuracy: 0.4705 - val_loss: 0.3923 - val_accuracy: 0.4687\n",
            "Epoch 101/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3891 - accuracy: 0.4707 - val_loss: 0.3922 - val_accuracy: 0.4686\n",
            "Epoch 102/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3870 - accuracy: 0.4709 - val_loss: 0.3870 - val_accuracy: 0.4690\n",
            "Epoch 103/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3851 - accuracy: 0.4710 - val_loss: 0.3868 - val_accuracy: 0.4691\n",
            "Epoch 104/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3835 - accuracy: 0.4710 - val_loss: 0.3834 - val_accuracy: 0.4682\n",
            "Epoch 105/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3819 - accuracy: 0.4711 - val_loss: 0.3846 - val_accuracy: 0.4683\n",
            "Epoch 106/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3802 - accuracy: 0.4712 - val_loss: 0.3802 - val_accuracy: 0.4685\n",
            "Epoch 107/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3791 - accuracy: 0.4713 - val_loss: 0.3827 - val_accuracy: 0.4700\n",
            "Epoch 108/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3782 - accuracy: 0.4712 - val_loss: 0.3831 - val_accuracy: 0.4710\n",
            "Epoch 109/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.3772 - accuracy: 0.4712 - val_loss: 0.3768 - val_accuracy: 0.4683\n",
            "Epoch 110/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3742 - accuracy: 0.4719 - val_loss: 0.3770 - val_accuracy: 0.4702\n",
            "Epoch 111/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3736 - accuracy: 0.4721 - val_loss: 0.3719 - val_accuracy: 0.4705\n",
            "Epoch 112/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3731 - accuracy: 0.4716 - val_loss: 0.3766 - val_accuracy: 0.4683\n",
            "Epoch 113/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3704 - accuracy: 0.4721 - val_loss: 0.3718 - val_accuracy: 0.4697\n",
            "Epoch 114/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3691 - accuracy: 0.4725 - val_loss: 0.3679 - val_accuracy: 0.4705\n",
            "Epoch 115/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3683 - accuracy: 0.4727 - val_loss: 0.3698 - val_accuracy: 0.4701\n",
            "Epoch 116/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3667 - accuracy: 0.4731 - val_loss: 0.3675 - val_accuracy: 0.4708\n",
            "Epoch 117/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3647 - accuracy: 0.4735 - val_loss: 0.3676 - val_accuracy: 0.4731\n",
            "Epoch 118/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.3628 - accuracy: 0.4736 - val_loss: 0.3638 - val_accuracy: 0.4724\n",
            "Epoch 119/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3615 - accuracy: 0.4741 - val_loss: 0.3628 - val_accuracy: 0.4722\n",
            "Epoch 120/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3601 - accuracy: 0.4746 - val_loss: 0.3632 - val_accuracy: 0.4719\n",
            "Epoch 121/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3584 - accuracy: 0.4749 - val_loss: 0.3602 - val_accuracy: 0.4718\n",
            "Epoch 122/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3569 - accuracy: 0.4754 - val_loss: 0.3595 - val_accuracy: 0.4726\n",
            "Epoch 123/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3553 - accuracy: 0.4756 - val_loss: 0.3553 - val_accuracy: 0.4739\n",
            "Epoch 124/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3544 - accuracy: 0.4759 - val_loss: 0.3563 - val_accuracy: 0.4746\n",
            "Epoch 125/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3526 - accuracy: 0.4762 - val_loss: 0.3579 - val_accuracy: 0.4733\n",
            "Epoch 126/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.3515 - accuracy: 0.4767 - val_loss: 0.3526 - val_accuracy: 0.4735\n",
            "Epoch 127/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3500 - accuracy: 0.4769 - val_loss: 0.3551 - val_accuracy: 0.4761\n",
            "Epoch 128/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.3483 - accuracy: 0.4772 - val_loss: 0.3569 - val_accuracy: 0.4710\n",
            "Epoch 129/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3475 - accuracy: 0.4770 - val_loss: 0.3580 - val_accuracy: 0.4747\n",
            "Epoch 130/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.3458 - accuracy: 0.4774 - val_loss: 0.3498 - val_accuracy: 0.4758\n",
            "Epoch 131/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3446 - accuracy: 0.4777 - val_loss: 0.3505 - val_accuracy: 0.4767\n",
            "Epoch 132/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3433 - accuracy: 0.4778 - val_loss: 0.3471 - val_accuracy: 0.4763\n",
            "Epoch 133/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3421 - accuracy: 0.4781 - val_loss: 0.3435 - val_accuracy: 0.4762\n",
            "Epoch 134/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3410 - accuracy: 0.4783 - val_loss: 0.3420 - val_accuracy: 0.4751\n",
            "Epoch 135/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.3394 - accuracy: 0.4788 - val_loss: 0.3406 - val_accuracy: 0.4757\n",
            "Epoch 136/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3384 - accuracy: 0.4789 - val_loss: 0.3411 - val_accuracy: 0.4759\n",
            "Epoch 137/200\n",
            "216/216 [==============================] - 17s 78ms/step - loss: 0.3371 - accuracy: 0.4794 - val_loss: 0.3411 - val_accuracy: 0.4770\n",
            "Epoch 138/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3362 - accuracy: 0.4795 - val_loss: 0.3418 - val_accuracy: 0.4778\n",
            "Epoch 139/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3359 - accuracy: 0.4796 - val_loss: 0.3360 - val_accuracy: 0.4766\n",
            "Epoch 140/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3344 - accuracy: 0.4796 - val_loss: 0.3406 - val_accuracy: 0.4782\n",
            "Epoch 141/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3338 - accuracy: 0.4800 - val_loss: 0.3361 - val_accuracy: 0.4783\n",
            "Epoch 142/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3334 - accuracy: 0.4800 - val_loss: 0.3391 - val_accuracy: 0.4775\n",
            "Epoch 143/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3325 - accuracy: 0.4801 - val_loss: 0.3347 - val_accuracy: 0.4772\n",
            "Epoch 144/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3340 - accuracy: 0.4794 - val_loss: 0.3394 - val_accuracy: 0.4782\n",
            "Epoch 145/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3302 - accuracy: 0.4802 - val_loss: 0.3352 - val_accuracy: 0.4773\n",
            "Epoch 146/200\n",
            "216/216 [==============================] - 17s 78ms/step - loss: 0.3285 - accuracy: 0.4803 - val_loss: 0.3335 - val_accuracy: 0.4766\n",
            "Epoch 147/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3273 - accuracy: 0.4803 - val_loss: 0.3326 - val_accuracy: 0.4769\n",
            "Epoch 148/200\n",
            "216/216 [==============================] - 15s 72ms/step - loss: 0.3263 - accuracy: 0.4804 - val_loss: 0.3283 - val_accuracy: 0.4782\n",
            "Epoch 149/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3251 - accuracy: 0.4806 - val_loss: 0.3279 - val_accuracy: 0.4788\n",
            "Epoch 150/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3236 - accuracy: 0.4809 - val_loss: 0.3251 - val_accuracy: 0.4792\n",
            "Epoch 151/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3226 - accuracy: 0.4808 - val_loss: 0.3254 - val_accuracy: 0.4787\n",
            "Epoch 152/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3215 - accuracy: 0.4809 - val_loss: 0.3249 - val_accuracy: 0.4775\n",
            "Epoch 153/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3203 - accuracy: 0.4810 - val_loss: 0.3229 - val_accuracy: 0.4782\n",
            "Epoch 154/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3188 - accuracy: 0.4811 - val_loss: 0.3231 - val_accuracy: 0.4804\n",
            "Epoch 155/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.3179 - accuracy: 0.4812 - val_loss: 0.3191 - val_accuracy: 0.4789\n",
            "Epoch 156/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3164 - accuracy: 0.4814 - val_loss: 0.3222 - val_accuracy: 0.4788\n",
            "Epoch 157/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3155 - accuracy: 0.4815 - val_loss: 0.3161 - val_accuracy: 0.4786\n",
            "Epoch 158/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3141 - accuracy: 0.4818 - val_loss: 0.3158 - val_accuracy: 0.4802\n",
            "Epoch 159/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3127 - accuracy: 0.4819 - val_loss: 0.3172 - val_accuracy: 0.4806\n",
            "Epoch 160/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3119 - accuracy: 0.4822 - val_loss: 0.3156 - val_accuracy: 0.4779\n",
            "Epoch 161/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3107 - accuracy: 0.4823 - val_loss: 0.3141 - val_accuracy: 0.4787\n",
            "Epoch 162/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3093 - accuracy: 0.4823 - val_loss: 0.3157 - val_accuracy: 0.4780\n",
            "Epoch 163/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3087 - accuracy: 0.4823 - val_loss: 0.3099 - val_accuracy: 0.4806\n",
            "Epoch 164/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3075 - accuracy: 0.4824 - val_loss: 0.3108 - val_accuracy: 0.4807\n",
            "Epoch 165/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.3069 - accuracy: 0.4826 - val_loss: 0.3111 - val_accuracy: 0.4792\n",
            "Epoch 166/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3058 - accuracy: 0.4825 - val_loss: 0.3111 - val_accuracy: 0.4786\n",
            "Epoch 167/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3047 - accuracy: 0.4829 - val_loss: 0.3066 - val_accuracy: 0.4805\n",
            "Epoch 168/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3035 - accuracy: 0.4830 - val_loss: 0.3064 - val_accuracy: 0.4817\n",
            "Epoch 169/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.3022 - accuracy: 0.4832 - val_loss: 0.3073 - val_accuracy: 0.4805\n",
            "Epoch 170/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.3015 - accuracy: 0.4832 - val_loss: 0.3053 - val_accuracy: 0.4814\n",
            "Epoch 171/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.3100 - accuracy: 0.4809 - val_loss: 0.3059 - val_accuracy: 0.4799\n",
            "Epoch 172/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2994 - accuracy: 0.4835 - val_loss: 0.3036 - val_accuracy: 0.4811\n",
            "Epoch 173/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2988 - accuracy: 0.4836 - val_loss: 0.3032 - val_accuracy: 0.4814\n",
            "Epoch 174/200\n",
            "216/216 [==============================] - 16s 72ms/step - loss: 0.2974 - accuracy: 0.4838 - val_loss: 0.3018 - val_accuracy: 0.4817\n",
            "Epoch 175/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2959 - accuracy: 0.4838 - val_loss: 0.2978 - val_accuracy: 0.4809\n",
            "Epoch 176/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2946 - accuracy: 0.4839 - val_loss: 0.3017 - val_accuracy: 0.4803\n",
            "Epoch 177/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.2936 - accuracy: 0.4841 - val_loss: 0.2978 - val_accuracy: 0.4820\n",
            "Epoch 178/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2928 - accuracy: 0.4840 - val_loss: 0.2962 - val_accuracy: 0.4814\n",
            "Epoch 179/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2914 - accuracy: 0.4842 - val_loss: 0.2960 - val_accuracy: 0.4820\n",
            "Epoch 180/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2905 - accuracy: 0.4843 - val_loss: 0.2939 - val_accuracy: 0.4815\n",
            "Epoch 181/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2898 - accuracy: 0.4845 - val_loss: 0.2940 - val_accuracy: 0.4820\n",
            "Epoch 182/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.2890 - accuracy: 0.4845 - val_loss: 0.2915 - val_accuracy: 0.4818\n",
            "Epoch 183/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2874 - accuracy: 0.4846 - val_loss: 0.2932 - val_accuracy: 0.4821\n",
            "Epoch 184/200\n",
            "216/216 [==============================] - 16s 76ms/step - loss: 0.2864 - accuracy: 0.4847 - val_loss: 0.2922 - val_accuracy: 0.4813\n",
            "Epoch 185/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2875 - accuracy: 0.4846 - val_loss: 0.2938 - val_accuracy: 0.4818\n",
            "Epoch 186/200\n",
            "216/216 [==============================] - 17s 77ms/step - loss: 0.2867 - accuracy: 0.4847 - val_loss: 0.2908 - val_accuracy: 0.4808\n",
            "Epoch 187/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2855 - accuracy: 0.4848 - val_loss: 0.2884 - val_accuracy: 0.4822\n",
            "Epoch 188/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2847 - accuracy: 0.4849 - val_loss: 0.2887 - val_accuracy: 0.4825\n",
            "Epoch 189/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2835 - accuracy: 0.4850 - val_loss: 0.2880 - val_accuracy: 0.4822\n",
            "Epoch 190/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2838 - accuracy: 0.4849 - val_loss: 0.2883 - val_accuracy: 0.4821\n",
            "Epoch 191/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2825 - accuracy: 0.4851 - val_loss: 0.2872 - val_accuracy: 0.4819\n",
            "Epoch 192/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2811 - accuracy: 0.4853 - val_loss: 0.2849 - val_accuracy: 0.4828\n",
            "Epoch 193/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2800 - accuracy: 0.4854 - val_loss: 0.2869 - val_accuracy: 0.4830\n",
            "Epoch 194/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2792 - accuracy: 0.4856 - val_loss: 0.2836 - val_accuracy: 0.4829\n",
            "Epoch 195/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2778 - accuracy: 0.4858 - val_loss: 0.2838 - val_accuracy: 0.4838\n",
            "Epoch 196/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2769 - accuracy: 0.4860 - val_loss: 0.2815 - val_accuracy: 0.4831\n",
            "Epoch 197/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2760 - accuracy: 0.4861 - val_loss: 0.2785 - val_accuracy: 0.4834\n",
            "Epoch 198/200\n",
            "216/216 [==============================] - 15s 71ms/step - loss: 0.2755 - accuracy: 0.4861 - val_loss: 0.2807 - val_accuracy: 0.4835\n",
            "Epoch 199/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2746 - accuracy: 0.4862 - val_loss: 0.2829 - val_accuracy: 0.4821\n",
            "Epoch 200/200\n",
            "216/216 [==============================] - 15s 70ms/step - loss: 0.2741 - accuracy: 0.4862 - val_loss: 0.2783 - val_accuracy: 0.4832\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def load_data(filename):\n",
        "    with open(filename, encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text.split('\\n')\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokenizer = Tokenizer(char_level=False)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer, tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "def pad(sequences, maxlen=None):\n",
        "    return pad_sequences(sequences, padding='post', maxlen=maxlen)\n",
        "\n",
        "english_sentences = load_data('/content/drive/MyDrive/small_vocab_en.txt')\n",
        "french_sentences = load_data('/content/drive/MyDrive/small_vocab_fr.txt')\n",
        "\n",
        "english_tokenizer, english_tokenized = tokenize(english_sentences)\n",
        "french_tokenizer, french_tokenized = tokenize(french_sentences)\n",
        "\n",
        "max_english_length = max(len(sentence) for sentence in english_tokenized)\n",
        "max_french_length = max(len(sentence) for sentence in french_tokenized)\n",
        "\n",
        "english_padded = pad(english_tokenized, maxlen=max_english_length)\n",
        "french_padded = pad(french_tokenized, maxlen=max_french_length)\n",
        "\n",
        "encoder_input_data = np.array(english_padded)\n",
        "decoder_input_data = np.array(french_padded)\n",
        "decoder_output_data = np.zeros_like(decoder_input_data)\n",
        "decoder_output_data[:, :-1] = decoder_input_data[:, 1:]\n",
        "\n",
        "embedding_size = 256\n",
        "lstm_units = 512\n",
        "dropout_rate = 0.5\n",
        "\n",
        "num_encoder_tokens = len(english_tokenizer.word_index) + 1\n",
        "num_decoder_tokens = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True, dropout=dropout_rate)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding_layer = Embedding(num_decoder_tokens, embedding_size)\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, dropout=dropout_rate)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "decoder_output_one_hot = np.zeros(\n",
        "    (len(french_sentences), max_french_length, num_decoder_tokens),\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "for i, sentence in enumerate(decoder_output_data):\n",
        "    for t, word_idx in enumerate(sentence):\n",
        "        if word_idx > 0:\n",
        "            decoder_output_one_hot[i, t, word_idx] = 1\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_output_one_hot,\n",
        "    batch_size=512,\n",
        "    epochs=200,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_embedding_output = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_output, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "reverse_french_tokenizer = {i: word for word, i in french_tokenizer.word_index.items()}\n"
      ]
    }
  ]
}